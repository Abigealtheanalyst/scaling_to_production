{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assigment, we will work with the *Forest Fire* data set. Please download the data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/162/forest+fires). Extract the data files into the subdirectory: `../data/fires/` (relative to `./src/`).\n",
    "\n",
    "## Objective\n",
    "\n",
    "+ The model objective is to predict the area affected by forest fires given the features set. \n",
    "+ The objective of this exercise is to assess your ability to construct and evaluate model pipelines.\n",
    "+ Please note: the instructions are not meant to be 100% prescriptive, but instead they are a set of minimum requirements. If you find predictive performance gains by applying additional steps, by all means show them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Description\n",
    "\n",
    "From the description file contained in the archive (`forestfires.names`), we obtain the following variable descriptions:\n",
    "\n",
    "1. X - x-axis spatial coordinate within the Montesinho park map: 1 to 9\n",
    "2. Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9\n",
    "3. month - month of the year: \"jan\" to \"dec\" \n",
    "4. day - day of the week: \"mon\" to \"sun\"\n",
    "5. FFMC - FFMC index from the FWI system: 18.7 to 96.20\n",
    "6. DMC - DMC index from the FWI system: 1.1 to 291.3 \n",
    "7. DC - DC index from the FWI system: 7.9 to 860.6 \n",
    "8. ISI - ISI index from the FWI system: 0.0 to 56.10\n",
    "9. temp - temperature in Celsius degrees: 2.2 to 33.30\n",
    "10. RH - relative humidity in %: 15.0 to 100\n",
    "11. wind - wind speed in km/h: 0.40 to 9.40 \n",
    "12. rain - outside rain in mm/m2 : 0.0 to 6.4 \n",
    "13. area - the burned area of the forest (in ha): 0.00 to 1090.84 \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Specific Tasks\n",
    "\n",
    "+ Construct four model pipelines, out of combinations of the following components:\n",
    "\n",
    "    + Preprocessors:\n",
    "\n",
    "        - A simple processor that only scales numeric variables and recodes categorical variables.\n",
    "        - A transformation preprocessor that scales numeric variables and applies a non-linear transformation.\n",
    "    \n",
    "    + Regressor:\n",
    "\n",
    "        - A baseline regressor, which could be a [K-nearest neighbours model](https://open.spotify.com/track/4R3AU2pjv8ge2siX1fVbZs?si=b2712f32da0e4358) or a simple [linear regression model](https://scikit-learn.org/stable/modules/linear_model.html)\n",
    "        - An advanced regressor of your choice (e.g., Random Forest, Neural Network, etc.)\n",
    "\n",
    "+ Evaluate tune and evaluate each of the four model pipelines. \n",
    "\n",
    "    - Select a [performance metric](https://scikit-learn.org/stable/modules/linear_model.html) out of the following options: explained variance, max error, root mean squared error (RMSE), mean absolute error (MAE), r-squared.\n",
    "    - *TIPS*: \n",
    "    \n",
    "        * Out of the suggested metrics above, [some are correlation metrics, but this is a prediction problem](https://www.tmwr.org/performance#performance). Choose wisely (and don't choose the incorrect options.) \n",
    "\n",
    "+ Select the best-performing model and explain its predictions.\n",
    "\n",
    "    - Provide local explanations.\n",
    "    - Obtain global explanations and recommend a variable selection strategy.\n",
    "\n",
    "+ Export your model as a pickle file.\n",
    "\n",
    "\n",
    "You can work on the Jupyter notebook, as this experiment is fairly short (no need to use sacred). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Assuming that the files `adult.data` and `adult.test` are in `../data/adult/`, then you can use the code below to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# reading the forestfires.csv file \n",
    "fires_dt = pd.read_csv('../../05_src/data/fires/forestfires.csv')\n",
    "\n",
    "# re-assign names\n",
    "fires_dt = fires_dt.rename(\n",
    "    columns = {'X':'coord_x', 'Y':'coord_y', 'month': 'month', 'day':'day', 'FFMC': 'ffmc', \n",
    "               'DMC': 'dmc', 'DC':'dc', 'ISI': 'isi', 'temp': 'temp', 'RH': 'rh',\n",
    "               'wind': 'wind', 'rain': 'rain', 'area': 'area'})\n",
    "\n",
    "# exploring \n",
    "print(fires_dt.info())\n",
    "print(fires_dt.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get X and Y\n",
    "\n",
    "Create the features data frame and target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = fires_dt.columns[fires_dt.columns != 'area']\n",
    "area = fires_dt.columns[fires_dt.columns == 'area']\n",
    "\n",
    "# data frame\n",
    "X = fires_dt[features]\n",
    "\n",
    "# data frame - target\n",
    "Y= fires_dt[area]\n",
    "\n",
    "print('Features data frame: ', X.shape)\n",
    "print('Target data frame: ', Y.shape)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Create two [Column Transformers](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html), called preproc1 and preproc2, with the following guidelines:\n",
    "\n",
    "- Numerical variables\n",
    "\n",
    "    * (Preproc 1 and 2) Scaling: use a scaling method of your choice (Standard, Robust, Min-Max). \n",
    "    * Preproc 2 only: \n",
    "        \n",
    "        + Choose a transformation for any of your input variables (or several of them). Evaluate if this transformation is convenient.\n",
    "        + The choice of scaler is up to you.\n",
    "\n",
    "- Categorical variables: \n",
    "    \n",
    "    * (Preproc 1 and 2) Apply [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) where appropriate.\n",
    "\n",
    "\n",
    "+ The only difference between preproc1 and preproc2 is the non-linear transformation of the numerical variables.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shap\n",
    "import contextlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproc 1\n",
    "\n",
    "Create preproc1 below.\n",
    "\n",
    "+ Numeric: scaled variables, no other transforms.\n",
    "+ Categorical: one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproc1\n",
    "# Define column names for different types of features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# numerical\n",
    "nums1 = Pipeline([ \n",
    "    ('scaler', StandardScaler(),) \n",
    "])\n",
    "\n",
    "# categorical\n",
    "cat1 = Pipeline([\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore')) \n",
    "])\n",
    "\n",
    "preproc1 = ColumnTransformer([\n",
    "    ('num_trasnforms', nums1, numeric_features), \n",
    "    ('cat_trasnforms', cat1, categorical_features) \n",
    "])\n",
    "\n",
    "preproc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproc 2\n",
    "\n",
    "Create preproc1 below.\n",
    "\n",
    "+ Numeric: scaled variables, non-linear transformation to one or more variables.\n",
    "+ Categorical: one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproc2\n",
    "\n",
    "nums1 = Pipeline([\n",
    "    ('scaler', StandardScaler())  # Step 1: Scale features using StandardScaler\n",
    "])\n",
    "\n",
    "nums2 = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Step 1: Scale features using StandardScaler\n",
    "    ('transform', PowerTransformer(method='yeo-johnson', standardize=False, copy=False))  # Step 2: Transform using Yeo-Johnson\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features\n",
    "cat2 = Pipeline([\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preproc2 = ColumnTransformer([\n",
    "    ('num_transforms_nums2', nums1, ['coord_x', 'coord_y', 'ffmc', 'dmc', 'dc', 'isi']),  # Apply nums2 to rest of the numeric variables\n",
    "    ('num_transforms_nums1', nums2, ['temp', 'rh', 'wind', 'rain']),  # Apply nums1 to a subset of numeric variables\n",
    "    ('cat_transforms', cat2, categorical_features)  # Apply cat2 to categorical features\n",
    "])\n",
    "\n",
    "preproc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "\n",
    "\n",
    "Create a [model pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html): \n",
    "\n",
    "+ Add a step labelled `preprocessing` and assign the Column Transformer from the previous section.\n",
    "+ Add a step labelled `regressor` and assign a regression model to it. \n",
    "\n",
    "## Regressor\n",
    "\n",
    "+ Use a regression model to perform a prediction. \n",
    "\n",
    "    - Choose a baseline regressor, tune it (if necessary) using grid search, and evaluate it using cross-validation.\n",
    "    - Choose a more advance regressor, tune it (if necessary) using grid search, and evaluate it using cross-validation.\n",
    "    - Both model choices are up to you, feel free to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline A = preproc1 + baseline\n",
    "# Create the regression pipeline with LinearRegression\n",
    "Pipeline_A = Pipeline([\n",
    "    ('preprocessing', preproc1),  # Preprocessing 1\n",
    "    ('regressor', LinearRegression())  # LinearRegression as the regressor\n",
    "])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training data\n",
    "cv_results = cross_validate(Pipeline_A, X_train, Y_train, cv=5,\n",
    "                            scoring=('neg_mean_squared_error', 'r2'),\n",
    "                            return_train_score=True)\n",
    "\n",
    "# Extract and print cross-validation results\n",
    "train_rmse_cv = np.sqrt(-cv_results['train_neg_mean_squared_error'])\n",
    "test_rmse_cv = np.sqrt(-cv_results['test_neg_mean_squared_error'])\n",
    "mean_train_rmse = np.mean(train_rmse_cv)\n",
    "mean_test_rmse = np.mean(test_rmse_cv)\n",
    "\n",
    "train_r2_cv = cv_results['train_r2']\n",
    "test_r2_cv = cv_results['test_r2']\n",
    "mean_train_r2 = np.mean(train_r2_cv)\n",
    "mean_test_r2 = np.mean(test_r2_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train RMSE of 43.440 indicates that, on average, the model’s predictions are off by approximately 43.440 units from the actual values in the training set. This suggests that the model has errors in predicting the target variable for the training data.\n",
    "\n",
    "And the test RMSE of 41.177 indicates that, on average, the model’s predictions are off by approximately 41.177 units from the actual values in the test set. This suggests that the model is not preforming well making accurate predictions.\n",
    "\n",
    "The negative \n",
    " value (-0.316) for the test dataset and high RMSE values indicate that the model is not performing well. It is likely underfitting and unable to capture the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline B = preproc2 + baseline\n",
    "# Create the regression pipeline with LinearRegression\n",
    "Pipeline_B = Pipeline([\n",
    "    ('preprocessing', preproc2),  # Preprocessing 2\n",
    "    ('regressor', LinearRegression())  # LinearRegression as the regressor\n",
    "])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training data\n",
    "cv_results = cross_validate(Pipeline_B, X_train, Y_train, cv=5,\n",
    "                            scoring=('neg_mean_squared_error', 'r2'),\n",
    "                            return_train_score=True)\n",
    "\n",
    "# Extract and print cross-validation results\n",
    "train_rmse_cv = np.sqrt(-cv_results['train_neg_mean_squared_error'])\n",
    "test_rmse_cv = np.sqrt(-cv_results['test_neg_mean_squared_error'])\n",
    "mean_train_rmse = np.mean(train_rmse_cv)\n",
    "mean_test_rmse = np.mean(test_rmse_cv)\n",
    "\n",
    "train_r2_cv = cv_results['train_r2']\n",
    "test_r2_cv = cv_results['test_r2']\n",
    "mean_train_r2 = np.mean(train_r2_cv)\n",
    "mean_test_r2 = np.mean(test_r2_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of the RSME results, pipeine B model's predictions have a relatively high error rate, with both training and test sets.\n",
    "\n",
    "Same than the previous pipeline Mean Test \n",
    " is negative (-0.24), this means that the model does not explain much of the variance in the data and is performing poorly in both test and training sets. Very similar than pipeline A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline C = preproc1 + advanced model\n",
    "# Create the regression pipeline\n",
    "Pipeline_C = Pipeline([\n",
    "    ('preprocessing', preproc1),  # Preprocessing steps\n",
    "    ('regressor', RandomForestRegressor())  # RandomForestRegressor as the regressor\n",
    "])\n",
    "Pipeline_C\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training data\n",
    "cv_results = cross_validate(Pipeline_C, X_train, Y_train, cv=5,\n",
    "                            scoring=('neg_mean_squared_error', 'r2'),\n",
    "                            return_train_score=True)\n",
    "\n",
    "# Extract and print cross-validation results\n",
    "train_rmse_cv = np.sqrt(-cv_results['train_neg_mean_squared_error'])\n",
    "test_rmse_cv = np.sqrt(-cv_results['test_neg_mean_squared_error'])\n",
    "mean_train_rmse = np.mean(train_rmse_cv)\n",
    "mean_test_rmse = np.mean(test_rmse_cv)\n",
    "\n",
    "train_r2_cv = cv_results['train_r2']\n",
    "test_r2_cv = cv_results['test_r2']\n",
    "mean_train_r2 = np.mean(train_r2_cv)\n",
    "mean_test_r2 = np.mean(test_r2_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline C perfor better than the previous two pipelines with a mean train RMSE value of 19.006, but the Mean Test RMSE of 48.999. While the training RMSE is relatively low, indicating good performance in predicting the training data, the test RMSE is much higher, suggesting that the model does not generalize well to new data and exhibits high variability in prediction errors across different folds.\n",
    "\n",
    "In term of the \n",
    " Mean Train R-squared was 0.820 suggests a good fit of the model to the training data. But was not the case for the testing set with an \n",
    " Mean Train R-squared of -1.873. Again ratifying a goot fit for the testing set but not foor the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline D = preproc2 + advanced model\n",
    "# Create the regression pipeline\n",
    "Pipeline_D = Pipeline([\n",
    "    ('preprocessing', preproc2),  # Preprocessing steps\n",
    "    ('regressor', RandomForestRegressor())  # RandomForestRegressor as the regressor\n",
    "])\n",
    "Pipeline_D\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform cross-validation on training data\n",
    "cv_results = cross_validate(Pipeline_D, X_train, Y_train, cv=5,\n",
    "                            scoring=('neg_mean_squared_error', 'r2'),\n",
    "                            return_train_score=True)\n",
    "\n",
    "# Extract and print cross-validation results\n",
    "train_rmse_cv = np.sqrt(-cv_results['train_neg_mean_squared_error'])\n",
    "test_rmse_cv = np.sqrt(-cv_results['test_neg_mean_squared_error'])\n",
    "mean_train_rmse = np.mean(train_rmse_cv)\n",
    "mean_test_rmse = np.mean(test_rmse_cv)\n",
    "\n",
    "train_r2_cv = cv_results['train_r2']\n",
    "test_r2_cv = cv_results['test_r2']\n",
    "mean_train_r2 = np.mean(train_r2_cv)\n",
    "mean_test_r2 = np.mean(test_r2_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Hyperparams\n",
    "\n",
    "+ Perform GridSearch on each of the four pipelines. \n",
    "+ Tune at least one hyperparameter per pipeline.\n",
    "+ Experiment with at least four value combinations per pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning Pipeline A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tunning Pipeline_A using grid search\n",
    "\n",
    "#Parameter grid\n",
    "param_grid_A = {\n",
    "    'regressor__fit_intercept': [True, False],  # Whether to calculate the intercept\n",
    "    'preprocessing__num_trasnforms__scaler__with_mean': [True, False],  # Centering the data\n",
    "    'preprocessing__num_trasnforms__scaler__with_std': [True, False],   # Scaling to unit variance\n",
    "}\n",
    "\n",
    "#G to test all values for n_neighbors\n",
    "pipe_gs_A = GridSearchCV(Pipeline_A, param_grid_A, cv=5, verbose=10)\n",
    "\n",
    "#fit model to data\n",
    "with open(os.devnull, 'w') as devnull:\n",
    "    with contextlib.redirect_stdout(devnull):\n",
    "        pipe_gs_A.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best parameters and best score\n",
    "print(\"Best parameters: \", pipe_gs_A.best_params_)\n",
    "print(\"Best score: \", pipe_gs_A.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning Pipeline B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid\n",
    "param_grid_B = {\n",
    "    'regressor__fit_intercept': [True, False],  # Whether to calculate the intercept\n",
    "    'preprocessing__num_transforms_nums2__scaler__with_mean': [True, False],  # Centering the data for num_transforms_nums2\n",
    "    'preprocessing__num_transforms_nums2__scaler__with_std': [True, False],   # Scaling to unit variance for num_transforms_nums2\n",
    "    'preprocessing__num_transforms_nums1__scaler__with_mean': [True, False],  # Centering the data for num_transforms_nums1\n",
    "    'preprocessing__num_transforms_nums1__scaler__with_std': [True, False],    # Scaling to unit variance for num_transforms_nums1\n",
    "    'preprocessing__num_transforms_nums1__transform__method': ['yeo-johnson', 'box-cox'],  # Method for PowerTransformer in num_transforms_nums1\n",
    "}\n",
    "\n",
    "#Gridsearch to test all values for n_neighbors\n",
    "pipe_gs_B = GridSearchCV(Pipeline_B, param_grid_B, cv=5, verbose=10)\n",
    "\n",
    "#fit model to data\n",
    "with open(os.devnull, 'w') as devnull:\n",
    "    with contextlib.redirect_stdout(devnull):\n",
    "        pipe_gs_B.fit(X, Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best parameters and best score\n",
    "print(\"Best parameters: \", pipe_gs_B.best_params_)\n",
    "print(\"Best score: \", pipe_gs_B.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning Pipeline C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameter grid\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [50, 100, 200],          # Number of trees in the forest\n",
    "    'regressor__max_features': ['auto', 'sqrt', 'log2'], # Number of features to consider at every split\n",
    "    'regressor__max_depth': [None, 10, 20, 30],          # Maximum number of levels in tree\n",
    "    'regressor__min_samples_split': [2, 5, 10],          # Minimum number of samples required to split a node\n",
    "    'regressor__min_samples_leaf': [1, 2, 4],            # Minimum number of samples required at each leaf node\n",
    "    'regressor__bootstrap': [True, False]                # Method of selecting samples for training each tree\n",
    "}\n",
    "#use gridsearch to test all values for n_neighbors\n",
    "pipe_gs_C = GridSearchCV(Pipeline_C, param_grid, cv=5, verbose=10)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "with open(os.devnull, 'w') as devnull:\n",
    "    with contextlib.redirect_stdout(devnull):\n",
    "        pipe_gs_C.fit(X, Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best parameters and best score\n",
    "print(\"Best parameters: \", pipe_gs_C.best_params_)\n",
    "print(\"Best score: \", pipe_gs_C.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "+ Which model has the best performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline C with RandomForestRegressor\n",
    "\n",
    "Best parameters: {'regressor__bootstrap': True, 'regressor__max_depth': 30, 'regressor__max_features': 'log2', 'regressor__min_samples_leaf': 4, 'regressor__min_samples_split': 5, 'regressor__n_estimators': 50}\n",
    "\n",
    "Best score: -0.15796425720033555"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export\n",
    "\n",
    "+ Save the best performing model to a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to save the model\n",
    "pipeline_D_file = 'pipeline_D.pkl'\n",
    "\n",
    "# Save the modelto a file\n",
    "with open(pipeline_D_file, 'wb') as file:\n",
    "    pickle.dump(pipeline_D_file, file)\n",
    "\n",
    "# Close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain\n",
    "\n",
    "+ Use SHAP values to explain the following only for the best-performing model:\n",
    "\n",
    "    - Select an observation in your test set and explain which are the most important features that explain that observation's specific prediction.\n",
    "\n",
    "    - In general, across the complete training set, which features are the most and least important.\n",
    "\n",
    "+ If you were to remove features from the model, which ones would you remove? Why? How would you test that these features are actually enhancing model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "data_transform = pipe_gs_D.best_estimator_.named_steps['preprocessing'].transform(X_test)\n",
    "\n",
    "explainer = shap.explainers.Tree(\n",
    "    pipe_gs_D.best_estimator_.named_steps['regressor'], \n",
    "    data_transform,\n",
    "    feature_names = pipe_gs_D.best_estimator_.named_steps['preprocessing'].get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer(data_transform, check_additivity = False)\n",
    "shap.plots.waterfall(shap_values[42], max_display = 30)\n",
    "shap.plots.beeswarm(shap_values, max_display = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Answer here.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The temperature, DMC (duff moisture code), coordinates, day Saturday, and FFMC (fine fuel moisture code) rank as the top features, according to these results.\n",
    "\n",
    "The following months are the least significant, as seen in the figures: January, November, April, May, and rain.\n",
    "\n",
    "Plotting the performance metrics (e.g., accuracy, RMSE) when utilising all features vs. selected features would allow us to see how the features affect the system.\n",
    "\n",
    "Since the months and the rain aspects don't seem to improve the model, I would delete them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "🚨 **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** 🚨 for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-3`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_3.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Cortez,Paulo and Morais,Anbal. (2008). Forest Fires. UCI Machine Learning Repository. https://doi.org/10.24432/C5D88D."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
