{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assigment, we will work with the *Adult* data set. Please download the data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/2/adult). Extract the data files into the subdirectory: `../05_src/data/adult/` (relative to `./05_src/`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Assuming that the files `adult.data` and `adult.test` are in `../05_src/data/adult/`, then you can use the code below to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "    'native-country', 'income'\n",
    "]\n",
    "adult_dt = (pd.read_csv('../05_src/data/adult/adult.data', header = None, names = columns)\n",
    "              .assign(income = lambda x: (x.income.str.strip() == '>50K')*1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get X and Y\n",
    "\n",
    "Create the features data frame and target data:\n",
    "\n",
    "+ Create a dataframe `X` that holds the features (all columns that are not `income`).\n",
    "+ Create a dataframe `Y` that holds the target data (`income`).\n",
    "+ From `X` and `Y`, obtain the training and testing data sets:\n",
    "\n",
    "    - Use a train-test split of 70-30%. \n",
    "    - Set the random state of the splitting function to 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring the data frame\n",
    "adult_dt.info()\n",
    "\n",
    "features = adult_dt.columns[adult_dt.columns != 'income']\n",
    "income = adult_dt.columns[adult_dt.columns == 'income']\n",
    "\n",
    "# Create DataFrame X with all columns except 'income'\n",
    "X = adult_dt[features]\n",
    "Y= adult_dt[income]\n",
    "\n",
    "#changing the column names to snake format (my preference)\n",
    "X = X.rename(columns={\n",
    "    'age': 'age',\n",
    "    'workclass': 'workclass',\n",
    "    'fnlwgt': 'fnlwgt',\n",
    "    'education': 'education',\n",
    "    'education-num': 'education_num',\n",
    "    'marital-status': 'marital_status',\n",
    "    'occupation': 'occupation',\n",
    "    'relationship': 'relationship',\n",
    "    'race': 'race',\n",
    "    'sex': 'sex',\n",
    "    'capital-gain': 'capital_gain',\n",
    "    'capital-loss': 'capital_loss',\n",
    "    'hours-per-week': 'hours_per_week',\n",
    "    'native-country': 'native_country'\n",
    "})\n",
    "\n",
    "# checking X and Y information\n",
    "print(X.info())\n",
    "print(Y.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random States\n",
    "\n",
    "Please comment: \n",
    "\n",
    "+ What is the [random state](https://scikit-learn.org/stable/glossary.html#term-random_state) of the [splitting function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)? \n",
    "+ Why is it [useful](https://en.wikipedia.org/wiki/Reproducibility)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Comment here.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "The random state is a parameter used in diferent functions to control the random number generation, the purpose of this is reproducibility.\n",
    "\n",
    "The splitting function is part of the sklearn.model_selection module, and it is used to separate/split the data into two random subsets (train and test).\n",
    "\n",
    "When we use the splitting function we set up a random state value to control the randomness involved in splitting the dataset.\n",
    "\n",
    "2. \n",
    "It is useful to be able to reproduce the same results with the same data set, this helps with reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Create a [Column Transformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) that treats the features as follows:\n",
    "\n",
    "- Numerical variables\n",
    "\n",
    "    * Apply [KNN-based imputation for completing missing values](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html):\n",
    "        \n",
    "        + Consider the 7 nearest neighbours.\n",
    "        + Weight each neighbour by the inverse of its distance, causing closer neigbours to have more influence than more distant ones.\n",
    "    * [Scale features using statistics that are robust to outliers](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler).\n",
    "\n",
    "- Categorical variables: \n",
    "    \n",
    "    * Apply a [simple imputation strategy](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer):\n",
    "\n",
    "        + Use the most frequent value to complete missing values, also called the *mode*.\n",
    "\n",
    "    * Apply [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html):\n",
    "        \n",
    "        + Handle unknown labels if they exist.\n",
    "        + Drop one column for binary variables.\n",
    "    \n",
    "    \n",
    "The column transformer should look like this:\n",
    "\n",
    "![](./images/assignment_2__column_transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical features (columns)\n",
    "num_cols = ['age', \n",
    "            'fnlwgt',\n",
    "            'education_num',\n",
    "            'capital_gain', \n",
    "            'capital_loss', \n",
    "            'hours_per_week']\n",
    "\n",
    "# categorical features (columns)\n",
    "cat_cols = ['workclass',\n",
    "            'education',\n",
    "            'marital_status',\n",
    "            'occupation',\n",
    "            'relationship',\n",
    "            'race',\n",
    "            'sex',\n",
    "            'native_country']\n",
    "\n",
    "# starting the pipeline\n",
    "# numerical features\n",
    "nums = Pipeline([\n",
    "    ('imputer', KNNImputer(n_neighbors=7, weights='distance')), # Distance-based imputation - KNNImputer\n",
    "    ('scaler', RobustScaler()) # Scale features using statistict robust to outliers - RobustScaler\n",
    "])\n",
    "\n",
    "# categorical features\n",
    "cat = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy = 'most_frequent')), # Impute missing values with the most frequent value - SimpleImputer\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', drop='if_binary')) # One-hot encode categorical features -OneHotEncoder\n",
    "])\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('num_trasnforms', nums, num_cols), # Apply num_transforms to num_cols\n",
    "    ('cat_trasnforms', cat, cat_cols) # Apply cat_transforms to cat_cols\n",
    "])\n",
    "\n",
    "preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "\n",
    "Create a [model pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html): \n",
    "\n",
    "+ Add a step labelled `preprocessing` and assign the Column Transformer from the previous section.\n",
    "+ Add a step labelled `classifier` and assign a [`RandomForestClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to it.\n",
    "\n",
    "The pipeline looks like this:\n",
    "\n",
    "![](./images/assignment_2__pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model pipeline steps\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing), # Applying preprocessing steps\n",
    "    ('classifier', RandomForestClassifier()) # RandomForestClassifier as the classifier\n",
    "])\n",
    "\n",
    "model_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "Evaluate the model pipeline using [`cross_validate()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html):\n",
    "\n",
    "+ Measure the following [preformance metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values): negative log loss, ROC AUC, accuracy, and balanced accuracy.\n",
    "+ Report the training and validation results. \n",
    "+ Use five folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 42) # splitig the data set into traing and testing sets, using a random state of 42\n",
    "\n",
    "performance = ['neg_log_loss', 'roc_auc', 'accuracy', 'balanced_accuracy'] # metric to evaluate performance\n",
    "\n",
    "results = cross_validate(model_pipeline, X_train, Y_train, cv = 5, scoring = performance) # cross validation using 5 folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the fold-level results as a pandas data frame and sorted by negative log loss of the test (validation) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results).assign(experiment = 1) # converting results to a pandas data fram and assingning numer 1 to this experiment\n",
    "results_sorted = results.sort_values('test_neg_log_loss') # sorting results by negative log loss\n",
    "results_sorted\n",
    "\n",
    "# the best negative log loss value is the one closest to zero, in this case -0.396471"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean of each metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sorted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the same performance metrics (negative log loss, ROC AUC, accuracy, and balanced accuracy) using the testing data `X_test` and `Y_test`. Display results as a dictionary.\n",
    "\n",
    "*Tip*: both, `roc_auc()` and `neg_log_loss()` will require prediction scores from `pipe.predict_proba()`. However, for `roc_auc()` you should only pass the last column `Y_pred_proba[:, 1]`. Use `Y_pred_proba` with `neg_log_loss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the pipeline on training data\n",
    "model_pipeline.fit(X_train, Y_train)\n",
    "\n",
    "# predictions\n",
    "Y_pred = model_pipeline.predict(X_test)\n",
    "Y_pred_proba = model_pipeline.predict_proba(X_test)\n",
    "\n",
    "# Evaluate metrics on test data\n",
    "results_test = {'log_loss_test': log_loss(Y_test, Y_pred_proba),\n",
    "                'roc_auc': roc_auc_score(Y_test, Y_pred_proba[:, 1]),\n",
    "                'accuracy': accuracy_score(Y_test, Y_pred),\n",
    "                'balanced_accuracy': balanced_accuracy_score(Y_test, Y_pred)\n",
    "                }\n",
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Recoding\n",
    "\n",
    "In the first code chunk of this document, we loaded the data and immediately recoded the target variable `income`. Why is this [convenient](https://scikit-learn.org/stable/modules/model_evaluation.html#binary-case)?\n",
    "\n",
    "The specific line was:\n",
    "\n",
    "```\n",
    "adult_dt = (pd.read_csv('../05_src/data/adult/adult.data', header = None, names = columns)\n",
    "              .assign(income = lambda x: (x.income.str.strip() == '>50K')*1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Answer here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we are recoding the values of the feature when is equal to >50K we assinged the value of 1, other wise it will be 0, this is convenient beacuse it helps the algorithms to process categorical data effectively, this improves model performance, and helps the integration into machine learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "The [rubric](./assignment_2_rubric_clean.xlsx) contains the criteria for assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-2`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_2.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Becker,Barry and Kohavi,Ronny. (1996). Adult. UCI Machine Learning Repository. https://doi.org/10.24432/C5XW20."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
